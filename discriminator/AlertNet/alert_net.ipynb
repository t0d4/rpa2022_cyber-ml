{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CICIDSDataset(Dataset):\n",
    "    \"\"\"CIC-IDS-2017 Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        features = self.dataframe.iloc[idx, :-1]\n",
    "        features = np.array(features).astype('float32')\n",
    "        label = self.dataframe.iloc[idx, -1]\n",
    "\n",
    "        sample = (features, label)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myToTensor:\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample: tuple):\n",
    "        features, label = sample\n",
    "        features = torch.from_numpy(features)\n",
    "        return (features, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV & Standardize features & Convert labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded classes: ['BENIGN' 'Bot' 'DDoS' 'DoS' 'FTP-Patator' 'PortScan' 'SSH-Patator' 'Web']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    60000\n",
       "4     7000\n",
       "3     6000\n",
       "2     6000\n",
       "5     6000\n",
       "6     5000\n",
       "7     2000\n",
       "1     1500\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dir_path = Path('../../datasets/CIC-IDS-2017/MachineLearningCSV/MachineLearningCVE')\n",
    "\n",
    "df_train = pd.read_csv(csv_dir_path / 'train.csv')\n",
    "df_test = pd.read_csv(csv_dir_path / 'test.csv')\n",
    "\n",
    "# Standardize features\n",
    "epsilon = 1e-7  # avoid zero division\n",
    "feature_columns = df_train.columns[df_train.columns != 'Label']\n",
    "df_train[feature_columns] = (df_train[feature_columns] - df_train[feature_columns].mean()) / (df_train[feature_columns].std() + epsilon)\n",
    "df_test[feature_columns] = (df_test[feature_columns] - df_test[feature_columns].mean()) / (df_test[feature_columns].std() + epsilon)\n",
    "\n",
    "# Convert categorical variables to discrete numbers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df_train['Label'])\n",
    "df_train['Label'], df_test['Label'] = encoder.transform(df_train['Label']), encoder.transform(df_test['Label'])\n",
    "\n",
    "print(f\"Encoded classes: {encoder.classes_}\")\n",
    "df_train['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that training data doesn't have NaN\n",
    "columns_with_nan = list(df_train.columns[df_train.isna().any()])\n",
    "assert columns_with_nan == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CICIDSDataset(\n",
    "    dataframe=df_train,\n",
    "    transform=myToTensor()\n",
    ")\n",
    "test_dataset = CICIDSDataset(\n",
    "    dataframe=df_test,\n",
    "    transform=myToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEVER FAIL TO SHUFFLE the dataset, as it is aligned at this point.\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(df_train.columns) - 1  # num_of_features = num_of_all_columns - num_of_class_label\n",
    "n_classes = len(encoder.classes_)\n",
    "\n",
    "class AlertNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlertNet, self).__init__()\n",
    "        self.FCN_units = [n_features, 1024, 768, 512, 256, 128] # n_features is for input layer\n",
    "\n",
    "        layers = []\n",
    "        for idx in range(len(self.FCN_units)-1):\n",
    "            layers += [\n",
    "                nn.Linear(self.FCN_units[idx], self.FCN_units[idx+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(self.FCN_units[idx+1]),\n",
    "                nn.Dropout(0.01)\n",
    "            ]\n",
    "\n",
    "        self.sequential_model = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(self.FCN_units[-1], n_classes),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.sequential_model(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlertNet(\n",
      "  (sequential_model): Sequential(\n",
      "    (0): Linear(in_features=78, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.01, inplace=False)\n",
      "    (4): Linear(in_features=1024, out_features=768, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.01, inplace=False)\n",
      "    (8): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.01, inplace=False)\n",
      "    (12): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.01, inplace=False)\n",
      "    (16): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.01, inplace=False)\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=8, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AlertNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader: DataLoader, model: nn.Module, loss_function, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    print(f\"[[ Train ]]\")\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # feed the data to the network\n",
    "        pred = model(X)\n",
    "        loss = loss_function(pred, y)\n",
    "        # adjust the weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 400 == 0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            # Accuracy should not be considered as metrics that represent how good this model behaves\n",
    "            # (because classes are substantially imbalanced), but here we're calculating accuracy for reference\n",
    "            accuracy = (pred.argmax(dim=1) == y).type(torch.float).sum().item() / torch.numel(y)\n",
    "            print(f\"| position of this batch: {current:>5d}/{size:>5d} |\")\n",
    "            print(f\"Loss: {loss:>7f}\")\n",
    "            # print(f\"Accuracy in this batch (for reference): {(100*accuracy):>0.1f}%\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader: DataLoader, model: nn.Module, loss_function, n_classes: int):\n",
    "    num_batches = len(dataloader)\n",
    "    print(f\"[[ Test ]]\")\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    predicted_labels_all = []\n",
    "    correct_labels_all = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # add correct labels to calculate F1 score later\n",
    "            correct_labels_all += y.tolist()\n",
    "            # make prediction\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            pred_labels = pred.argmax(dim=1)\n",
    "            predicted_labels_all += pred_labels.tolist()\n",
    "            # accumulate the output of loss function\n",
    "            test_loss += loss_function(pred, y).item()\n",
    "        test_loss /= num_batches\n",
    "        f1_score_calculator = torchmetrics.F1Score(num_classes=n_classes, average='weighted')\n",
    "        f1_score = f1_score_calculator(\n",
    "            torch.from_numpy(\n",
    "                np.array(predicted_labels_all)\n",
    "            ),\n",
    "            torch.from_numpy(\n",
    "                np.array(correct_labels_all)\n",
    "            )\n",
    "        )\n",
    "        print(f\"F1 score (weighted): {f1_score}\")\n",
    "        print(f\"Average loss: {test_loss:>8f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ Epoch 1 ------------------------------\n",
      "[[ Train ]]\n",
      "| position of this batch:     0/93500 |\n",
      "Loss: 2.307563\n",
      "\n",
      "| position of this batch: 25600/93500 |\n",
      "Loss: 0.136688\n",
      "\n",
      "| position of this batch: 51200/93500 |\n",
      "Loss: 0.150344\n",
      "\n",
      "| position of this batch: 76800/93500 |\n",
      "Loss: 0.121287\n",
      "\n",
      "[[ Test ]]\n",
      "F1 score (weighted): 0.9751631021499634\n",
      "Average loss: 0.066150\n",
      "\n",
      "------------------------------ Epoch 2 ------------------------------\n",
      "[[ Train ]]\n",
      "| position of this batch:     0/93500 |\n",
      "Loss: 0.160525\n",
      "\n",
      "| position of this batch: 25600/93500 |\n",
      "Loss: 0.060201\n",
      "\n",
      "| position of this batch: 51200/93500 |\n",
      "Loss: 0.015956\n",
      "\n",
      "| position of this batch: 76800/93500 |\n",
      "Loss: 0.029680\n",
      "\n",
      "[[ Test ]]\n",
      "F1 score (weighted): 0.9790273904800415\n",
      "Average loss: 0.055066\n",
      "\n",
      "------------------------------ Epoch 3 ------------------------------\n",
      "[[ Train ]]\n",
      "| position of this batch:     0/93500 |\n",
      "Loss: 0.028439\n",
      "\n",
      "| position of this batch: 25600/93500 |\n",
      "Loss: 0.160505\n",
      "\n",
      "| position of this batch: 51200/93500 |\n",
      "Loss: 0.026715\n",
      "\n",
      "| position of this batch: 76800/93500 |\n",
      "Loss: 0.007983\n",
      "\n",
      "[[ Test ]]\n",
      "F1 score (weighted): 0.9848703742027283\n",
      "Average loss: 0.042423\n",
      "\n",
      "------------------------------ Epoch 4 ------------------------------\n",
      "[[ Train ]]\n",
      "| position of this batch:     0/93500 |\n",
      "Loss: 0.037859\n",
      "\n",
      "| position of this batch: 25600/93500 |\n",
      "Loss: 0.176898\n",
      "\n",
      "| position of this batch: 51200/93500 |\n",
      "Loss: 0.030475\n",
      "\n",
      "| position of this batch: 76800/93500 |\n",
      "Loss: 0.053957\n",
      "\n",
      "[[ Test ]]\n",
      "F1 score (weighted): 0.9843389391899109\n",
      "Average loss: 0.055962\n",
      "\n",
      "------------------------------ Epoch 5 ------------------------------\n",
      "[[ Train ]]\n",
      "| position of this batch:     0/93500 |\n",
      "Loss: 0.011001\n",
      "\n",
      "| position of this batch: 25600/93500 |\n",
      "Loss: 0.038975\n",
      "\n",
      "| position of this batch: 51200/93500 |\n",
      "Loss: 0.009786\n",
      "\n",
      "| position of this batch: 76800/93500 |\n",
      "Loss: 0.020314\n",
      "\n",
      "[[ Test ]]\n",
      "F1 score (weighted): 0.9855623841285706\n",
      "Average loss: 0.056121\n",
      "\n",
      "------------------------------ Epoch 6 ------------------------------\n",
      "[[ Train ]]\n",
      "| position of this batch:     0/93500 |\n",
      "Loss: 0.034281\n",
      "\n",
      "| position of this batch: 25600/93500 |\n",
      "Loss: 0.021033\n",
      "\n",
      "| position of this batch: 51200/93500 |\n",
      "Loss: 0.013959\n",
      "\n",
      "| position of this batch: 76800/93500 |\n",
      "Loss: 0.159660\n",
      "\n",
      "[[ Test ]]\n",
      "F1 score (weighted): 0.9848234057426453\n",
      "Average loss: 0.041499\n",
      "\n",
      "------------------------------ Epoch 7 ------------------------------\n",
      "[[ Train ]]\n",
      "| position of this batch:     0/93500 |\n",
      "Loss: 0.025117\n",
      "\n",
      "| position of this batch: 25600/93500 |\n",
      "Loss: 0.054028\n",
      "\n",
      "| position of this batch: 51200/93500 |\n",
      "Loss: 0.004293\n",
      "\n",
      "| position of this batch: 76800/93500 |\n",
      "Loss: 0.014246\n",
      "\n",
      "[[ Test ]]\n",
      "F1 score (weighted): 0.9864789247512817\n",
      "Average loss: 0.040747\n",
      "\n",
      "------------------------------ Epoch 8 ------------------------------\n",
      "[[ Train ]]\n",
      "| position of this batch:     0/93500 |\n",
      "Loss: 0.046724\n",
      "\n",
      "| position of this batch: 25600/93500 |\n",
      "Loss: 0.014514\n",
      "\n",
      "| position of this batch: 51200/93500 |\n",
      "Loss: 0.015622\n",
      "\n",
      "| position of this batch: 76800/93500 |\n",
      "Loss: 0.110801\n",
      "\n",
      "[[ Test ]]\n",
      "F1 score (weighted): 0.9882277846336365\n",
      "Average loss: 0.042822\n",
      "\n",
      "------------------------------ Epoch 9 ------------------------------\n",
      "[[ Train ]]\n",
      "| position of this batch:     0/93500 |\n",
      "Loss: 0.011422\n",
      "\n",
      "| position of this batch: 25600/93500 |\n",
      "Loss: 0.020389\n",
      "\n",
      "| position of this batch: 51200/93500 |\n",
      "Loss: 0.035983\n",
      "\n",
      "| position of this batch: 76800/93500 |\n",
      "Loss: 0.054976\n",
      "\n",
      "[[ Test ]]\n",
      "F1 score (weighted): 0.9867627024650574\n",
      "Average loss: 0.036827\n",
      "\n",
      "------------------------------ Epoch 10 ------------------------------\n",
      "[[ Train ]]\n",
      "| position of this batch:     0/93500 |\n",
      "Loss: 0.014448\n",
      "\n",
      "| position of this batch: 25600/93500 |\n",
      "Loss: 0.008081\n",
      "\n",
      "| position of this batch: 51200/93500 |\n",
      "Loss: 0.015794\n",
      "\n",
      "| position of this batch: 76800/93500 |\n",
      "Loss: 0.047910\n",
      "\n",
      "[[ Test ]]\n",
      "F1 score (weighted): 0.9874343872070312\n",
      "Average loss: 0.043207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f\"------------------------------ Epoch {epoch} ------------------------------\")\n",
    "    train(train_dataloader, model, loss_function, optimizer)\n",
    "    test(test_dataloader, model, loss_function, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'alert_net_state_dict.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea3fc519211cdd02c82251fa6301be607faf1a7144b4fc64372dbc862b32978f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
