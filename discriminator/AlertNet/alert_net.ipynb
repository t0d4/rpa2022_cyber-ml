{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CICIDSDataset(Dataset):\n",
    "    \"\"\"CIC-IDS-2017 Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        features = self.dataframe.iloc[idx, :-1]\n",
    "        features = np.array(features).astype('float32')\n",
    "        label = self.dataframe.iloc[idx, -1]\n",
    "\n",
    "        sample = (features, label)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myToTensor:\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample: tuple):\n",
    "        features, label = sample\n",
    "        features = torch.from_numpy(features)\n",
    "        return (features, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV & Standardize features & Convert labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded classes: ['BENIGN' 'Bot' 'DDoS' 'DoS' 'FTP-Patator' 'PortScan' 'SSH-Patator' 'Web']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    60000\n",
       "4     7000\n",
       "3     6000\n",
       "2     6000\n",
       "5     6000\n",
       "6     5000\n",
       "7     2000\n",
       "1     1500\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dir_path = Path('../../datasets/CIC-IDS-2017/MachineLearningCSV/MachineLearningCVE')\n",
    "\n",
    "df_train = pd.read_csv(csv_dir_path / 'train.csv')\n",
    "df_test = pd.read_csv(csv_dir_path / 'test.csv')\n",
    "\n",
    "# Standardize features\n",
    "epsilon = 1e-7  # avoid zero division\n",
    "feature_columns = df_train.columns[df_train.columns != 'Label']\n",
    "df_train[feature_columns] = (df_train[feature_columns] - df_train[feature_columns].mean()) / (df_train[feature_columns].std() + epsilon)\n",
    "df_test[feature_columns] = (df_test[feature_columns] - df_test[feature_columns].mean()) / (df_test[feature_columns].std() + epsilon)\n",
    "\n",
    "# Convert categorical variables to discrete numbers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df_train['Label'])\n",
    "df_train['Label'], df_test['Label'] = encoder.transform(df_train['Label']), encoder.transform(df_test['Label'])\n",
    "\n",
    "print(f\"Encoded classes: {encoder.classes_}\")\n",
    "df_train['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nan = list(df_train.columns[df_train.isna().any()])\n",
    "assert columns_with_nan == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CICIDSDataset(\n",
    "    dataframe=df_train,\n",
    "    transform=myToTensor()\n",
    ")\n",
    "test_dataset = CICIDSDataset(\n",
    "    dataframe=df_test,\n",
    "    transform=myToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEVER FAIL TO SHUFFLE the dataset, as it is aligned at this point.\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(df_train.columns) - 1  # num_of_features = num_of_all_columns - num_of_class_label\n",
    "n_classes = len(encoder.classes_)\n",
    "\n",
    "class AlertNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlertNet, self).__init__()\n",
    "        self.FCN_units = [n_features, 1024, 768, 512, 256, 128] # n_features is for input layer\n",
    "\n",
    "        layers = []\n",
    "        for idx in range(len(self.FCN_units)-1):\n",
    "            layers += [\n",
    "                nn.Linear(self.FCN_units[idx], self.FCN_units[idx+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(self.FCN_units[idx+1]),\n",
    "                nn.Dropout(0.01)\n",
    "            ]\n",
    "\n",
    "        self.sequential_model = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(self.FCN_units[-1], n_classes),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.sequential_model(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlertNet(\n",
      "  (sequential_model): Sequential(\n",
      "    (0): Linear(in_features=78, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.01, inplace=False)\n",
      "    (4): Linear(in_features=1024, out_features=768, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.01, inplace=False)\n",
      "    (8): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.01, inplace=False)\n",
      "    (12): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.01, inplace=False)\n",
      "    (16): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.01, inplace=False)\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=8, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AlertNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader: DataLoader, model: nn.Module, loss_function, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    print(f\"[[ Train ]]\")\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # feed the data to the network\n",
    "        pred = model(X)\n",
    "        loss = loss_function(pred, y)\n",
    "        # adjust the weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 400 == 0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            # Accuracy should not be considered as metrics that represent how good this model behaves\n",
    "            # (because classes are substantially imbalanced), but here we're calculating accuracy for reference\n",
    "            accuracy = (pred.argmax(dim=1) == y).type(torch.float).sum().item() / torch.numel(y)\n",
    "            print(f\"| position of this batch: {current:>5d}/{size:>5d} |\")\n",
    "            print(f\"Loss: {loss:>7f}\")\n",
    "            # print(f\"Accuracy in this batch (for reference): {(100*accuracy):>0.1f}%\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader: DataLoader, model: nn.Module, loss_function, n_classes: int):\n",
    "    num_batches = len(dataloader)\n",
    "    print(f\"[[ Test ]]\")\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    predicted_labels_all = []\n",
    "    correct_labels_all = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # add correct labels to calculate F1 score later\n",
    "            correct_labels_all += y.tolist()\n",
    "            # make prediction\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            pred_labels = pred.argmax(dim=1)\n",
    "            predicted_labels_all += pred_labels.tolist()\n",
    "            # accumulate the output of loss function\n",
    "            test_loss += loss_function(pred, y).item()\n",
    "        test_loss /= num_batches\n",
    "        f1_score_calculator = torchmetrics.F1Score(num_classes=n_classes, average='weighted')\n",
    "        f1_score = f1_score_calculator(\n",
    "            torch.from_numpy(\n",
    "                np.array(predicted_labels_all)\n",
    "            ),\n",
    "            torch.from_numpy(\n",
    "                np.array(correct_labels_all)\n",
    "            )\n",
    "        )\n",
    "        print(f\"F1 score (weighted): {f1_score}\")\n",
    "        print(f\"Average loss: {test_loss:>8f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1 ---\n",
      "[ Train ]\n",
      "| batch progress:     0/93500 |\n",
      "Loss: 2.251370\n",
      "Accuracy (for reference): 9.4%\n",
      "| batch progress:  6400/93500 |\n",
      "Loss: 0.112688\n",
      "Accuracy (for reference): 93.8%\n",
      "| batch progress: 12800/93500 |\n",
      "Loss: 0.181254\n",
      "Accuracy (for reference): 96.9%\n",
      "| batch progress: 19200/93500 |\n",
      "Loss: 0.152173\n",
      "Accuracy (for reference): 95.3%\n",
      "| batch progress: 25600/93500 |\n",
      "Loss: 0.106464\n",
      "Accuracy (for reference): 96.9%\n",
      "| batch progress: 32000/93500 |\n",
      "Loss: 0.037987\n",
      "Accuracy (for reference): 100.0%\n",
      "| batch progress: 38400/93500 |\n",
      "Loss: 0.121900\n",
      "Accuracy (for reference): 96.9%\n",
      "| batch progress: 44800/93500 |\n",
      "Loss: 0.116409\n",
      "Accuracy (for reference): 96.9%\n",
      "| batch progress: 51200/93500 |\n",
      "Loss: 0.018553\n",
      "Accuracy (for reference): 100.0%\n",
      "| batch progress: 57600/93500 |\n",
      "Loss: 0.142110\n",
      "Accuracy (for reference): 93.8%\n",
      "| batch progress: 64000/93500 |\n",
      "Loss: 0.009108\n",
      "Accuracy (for reference): 100.0%\n",
      "| batch progress: 70400/93500 |\n",
      "Loss: 0.059976\n",
      "Accuracy (for reference): 100.0%\n",
      "| batch progress: 76800/93500 |\n",
      "Loss: 0.041195\n",
      "Accuracy (for reference): 100.0%\n",
      "| batch progress: 83200/93500 |\n",
      "Loss: 0.021197\n",
      "Accuracy (for reference): 100.0%\n",
      "| batch progress: 89600/93500 |\n",
      "Loss: 0.009685\n",
      "Accuracy (for reference): 100.0%\n",
      "[ Test ]\n",
      "F1 score (weighted): 0.9666310548782349\n",
      "Average loss: 0.074538\n",
      "--- Epoch 2 ---\n",
      "[ Train ]\n",
      "| batch progress:     0/93500 |\n",
      "Loss: 0.063251\n",
      "Accuracy (for reference): 98.4%\n",
      "| batch progress:  6400/93500 |\n",
      "Loss: 0.005646\n",
      "Accuracy (for reference): 100.0%\n",
      "| batch progress: 12800/93500 |\n",
      "Loss: 0.087008\n",
      "Accuracy (for reference): 95.3%\n",
      "| batch progress: 19200/93500 |\n",
      "Loss: 0.183099\n",
      "Accuracy (for reference): 95.3%\n",
      "| batch progress: 25600/93500 |\n",
      "Loss: 0.055441\n",
      "Accuracy (for reference): 96.9%\n",
      "| batch progress: 32000/93500 |\n",
      "Loss: 0.014713\n",
      "Accuracy (for reference): 100.0%\n",
      "| batch progress: 38400/93500 |\n",
      "Loss: 0.051453\n",
      "Accuracy (for reference): 98.4%\n",
      "| batch progress: 44800/93500 |\n",
      "Loss: 0.188904\n",
      "Accuracy (for reference): 95.3%\n",
      "| batch progress: 51200/93500 |\n",
      "Loss: 0.011932\n",
      "Accuracy (for reference): 100.0%\n",
      "| batch progress: 57600/93500 |\n",
      "Loss: 0.060487\n",
      "Accuracy (for reference): 96.9%\n",
      "| batch progress: 64000/93500 |\n",
      "Loss: 0.063147\n",
      "Accuracy (for reference): 98.4%\n",
      "| batch progress: 70400/93500 |\n",
      "Loss: 0.021212\n",
      "Accuracy (for reference): 100.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb#ch0000025?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb#ch0000025?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m--- Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m ---\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb#ch0000025?line=3'>4</a>\u001b[0m     train(train_dataloader, model, loss_function, optimizer)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb#ch0000025?line=4'>5</a>\u001b[0m     test(test_dataloader, model, loss_function, n_classes)\n",
      "\u001b[1;32m/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb Cell 21'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_function, optimizer)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb#ch0000021?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[ Train ]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb#ch0000021?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb#ch0000021?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb#ch0000021?line=6'>7</a>\u001b[0m     X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb#ch0000021?line=7'>8</a>\u001b[0m     \u001b[39m# feed the data to the network\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb Cell 4'\u001b[0m in \u001b[0;36mCICIDSDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb#ch0000003?line=11'>12</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_tensor(idx):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb#ch0000003?line=12'>13</a>\u001b[0m     idx \u001b[39m=\u001b[39m idx\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb#ch0000003?line=14'>15</a>\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataframe\u001b[39m.\u001b[39;49miloc[idx, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb#ch0000003?line=15'>16</a>\u001b[0m features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(features)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/takaaki/Works/rpa2022_cyber-ml/discriminator/AlertNet/alert_net.ipynb#ch0000003?line=16'>17</a>\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataframe\u001b[39m.\u001b[39miloc[idx, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py:961\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=958'>959</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=959'>960</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[0;32m--> <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=960'>961</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=961'>962</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=962'>963</a>\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=963'>964</a>\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py:1458\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=1455'>1456</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_getitem_tuple\u001b[39m(\u001b[39mself\u001b[39m, tup: \u001b[39mtuple\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=1457'>1458</a>\u001b[0m     tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_tuple_indexer(tup)\n\u001b[1;32m   <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=1458'>1459</a>\u001b[0m     \u001b[39mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=1459'>1460</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_lowerdim(tup)\n",
      "File \u001b[0;32m~/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py:769\u001b[0m, in \u001b[0;36m_LocationIndexer._validate_tuple_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=766'>767</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, k \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(key):\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=767'>768</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=768'>769</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_key(k, i)\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=769'>770</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=770'>771</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=771'>772</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLocation based indexing can only have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=772'>773</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_valid_types\u001b[39m}\u001b[39;00m\u001b[39m] types\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/takaaki/.virtualenv/torch/lib/python3.8/site-packages/pandas/core/indexing.py?line=773'>774</a>\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f\"------------------------------ Epoch {epoch} ------------------------------\")\n",
    "    train(train_dataloader, model, loss_function, optimizer)\n",
    "    test(test_dataloader, model, loss_function, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'alert_net_state_dict.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea3fc519211cdd02c82251fa6301be607faf1a7144b4fc64372dbc862b32978f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
